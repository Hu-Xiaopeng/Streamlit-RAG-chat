import streamlit as st
import os
import tempfile
import hashlib
from llama_cpp import Llama
import numpy as np
from typing import List
import jieba
import gc
import PyPDF2
import docx
import markdown
from bs4 import BeautifulSoup



# é¢„è®¾çš„åµŒå…¥æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹
PRESET_EMBED_MODELS = [
    {"name": "bge-small-en-v1_5-f16.gguf", "path": "./bge-small-en-v1_5-f16.gguf"},
    # å¯ä»¥æ·»åŠ æ›´å¤šçš„é¢„è®¾æ¨¡å‹
]
PRESET_LLM_MODELS = [
    {"name": "gemma-2-2B-it-Q4_0_4_4.gguf", "path": "./gemma-2-2B-it-Q4_0_4_4.gguf"},
    # å¯ä»¥æ·»åŠ æ›´å¤šçš„é¢„è®¾æ¨¡å‹
]

# å¤„ç†æ–‡ä»¶ä¸Šä¼ 
def handle_file_upload(uploaded_files):
    if uploaded_files:
        temp_dir = tempfile.mkdtemp()
        for uploaded_file in uploaded_files:
            file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getvalue())
        return temp_dir
    return None

# è®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„å“ˆå¸Œå€¼
def get_files_hash(files):
    hash_md5 = hashlib.md5()
    for file in files:
        file_bytes = file.read()
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        hash_md5.update(file_bytes)
    return hash_md5.hexdigest()



############################################################
def extract_text_from_pdf(file_path: str):
    """Extract text content from a PDF file."""
    contents = []
    with open(file_path, 'rb') as f:
        pdf_reader = PyPDF2.PdfReader(f)
        for page in pdf_reader.pages:
            page_text = page.extract_text().strip()
            raw_text = [text.strip() for text in page_text.splitlines() if text.strip()]
            new_text = ''
            for text in raw_text:
                new_text += text
                if text[-1] in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', ';', 'ï¼›', ':', 'ï¼š', 'â€', 'â€™', 'ï¼‰', 'ã€‘', 'ã€‹', 'ã€', 'ã€', 'ã€•', 'ã€‰', 'ã€‹', 'ã€—', 'ã€', 'ã€Ÿ', 'Â»', '"', "'", ')', ']', '}']:
                    contents.append(new_text)
                    new_text = ''
            if new_text:
                contents.append(new_text)
    return contents

def extract_text_from_txt(file_path: str):
    """Extract text content from a TXT file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        contents = [text.strip() for text in f.readlines() if text.strip()]
    return contents

def extract_text_from_docx(file_path: str):
    """Extract text content from a DOCX file."""
    document = docx.Document(file_path)
    contents = [paragraph.text.strip() for paragraph in document.paragraphs if paragraph.text.strip()]
    return contents

def extract_text_from_markdown(file_path: str):
    """Extract text content from a Markdown file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        markdown_text = f.read()
    html = markdown.markdown(markdown_text)
    soup = BeautifulSoup(html, 'html.parser')
    contents = [text.strip() for text in soup.get_text().splitlines() if text.strip()]
    return contents

def preprocess_chunks(doc_file: str, chunks, start_index=0):
    chunks_with_info = []
    if len(chunks) < 1:
        return chunks_with_info
    index = start_index
    for chunk in chunks:
        current = {}
        current["metadata"] = {}
        current["metadata"]["source"] = doc_file
        current["metadata"]["chunk_id"] = index
        current["content"] = chunk
        chunks_with_info.append(current)
        index = index + 1
    return chunks_with_info

class SentenceSplitter:
    def __init__(self, chunk_size: int = 250, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str) -> List[str]:
        if self._is_has_chinese(text):
            return self._split_chinese_text(text)
        else:
            return self._split_english_text(text)

    def _split_chinese_text(self, text: str) -> List[str]:
        sentence_endings = {'\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'â€¦'}  # å¥æœ«æ ‡ç‚¹ç¬¦å·
        chunks, current_chunk = [], ''
        for word in jieba.cut(text):
            if len(current_chunk) + len(word) > self.chunk_size:
                chunks.append(current_chunk.strip())
                current_chunk = word
            else:
                current_chunk += word
            if word[-1] in sentence_endings and len(current_chunk) > self.chunk_size - self.chunk_overlap:
                chunks.append(current_chunk.strip())
                current_chunk = ''
        if current_chunk:
            chunks.append(current_chunk.strip())
        if self.chunk_overlap > 0 and len(chunks) > 1:
            chunks = self._handle_overlap(chunks)
        return chunks

    def _split_english_text(self, text: str) -> List[str]:
        sentences = re.split(r'(?<=[.!?])\s+', text.replace('\n', ' '))
        chunks, current_chunk = [], ''
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= self.chunk_size or not current_chunk:
                current_chunk += (' ' if current_chunk else '') + sentence
            else:
                chunks.append(current_chunk)
                current_chunk = sentence
        if current_chunk:  # Add the last chunk
            chunks.append(current_chunk)
        if self.chunk_overlap > 0 and len(chunks) > 1:
            chunks = self._handle_overlap(chunks)
        return chunks

    def _is_has_chinese(self, text: str) -> bool:
        if any("\u4e00" <= ch <= "\u9fff" for ch in text):
            return True
        else:
            return False

    def _handle_overlap(self, chunks: List[str]) -> List[str]:
        overlapped_chunks = []
        for i in range(len(chunks) - 1):
            chunk = chunks[i] + ' ' + chunks[i + 1][:self.chunk_overlap]
            overlapped_chunks.append(chunk.strip())
        overlapped_chunks.append(chunks[-1])
        return overlapped_chunks


class Kb:
    # åˆå§‹åŒ–çŸ¥è¯†åº“ï¼Œä»å¤šä¸ªæ–‡ä»¶ä¸­è¯»å–å†…å®¹
    def __init__(self, filepaths, llm_embed, chunk_size=250, chunk_overlap=50):
        self.docs = []
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        # æ£€æŸ¥ filepaths æ˜¯å¦ä¸º None
        if filepaths is not None:
            for filepath in filepaths:
                if filepath.endswith('.pdf'):
                    content = extract_text_from_pdf(filepath)
                elif filepath.endswith('.docx'):
                    content = extract_text_from_docx(filepath)
                elif filepath.endswith('.md'):
                    content = extract_text_from_markdown(filepath)
                else:
                    content = extract_text_from_txt(filepath)
                self.docs.extend(self.split_content(content))
            self.embeds = self.embed(self.docs, llm_embed)
        else:
            # filepaths ä¸º None æ—¶çš„å¤„ç†é€»è¾‘
            # ä¾‹å¦‚ï¼Œå¯ä»¥æ‰“å°ä¸€æ¡æ¶ˆæ¯ï¼Œæˆ–è€…åˆå§‹åŒ–ä¸ºç©ºçš„çŸ¥è¯†åº“
            print("No file paths provided. The knowledge base will be empty.")

    # æ ¹æ®ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦ï¼Œå°†çŸ¥è¯†åº“å†…å®¹åˆ†å‰²æˆå¤šä¸ªchunks
    def split_content(self, content):
        text_splitter = SentenceSplitter(self.chunk_size, self.chunk_overlap)  # ä½¿ç”¨å®ä¾‹å˜é‡
        chunks = text_splitter.split_text('\n'.join(content))
        return chunks

    # ä¸ºäº†æ–¹ä¾¿æ–‡æ¡£chunksè¿›è¡Œæ£€ç´¢ï¼Œæ‰€ä»¥ä¼šä½¿ç”¨embeddingæ¨¡å‹å°†chunksè½¬æ¢ä¸ºå‘é‡
    def embed(self, texts, llm_embed):
        embeds = []
        for text in texts:
            embeddings = llm_embed.create_embedding(text)
            if 'data' in embeddings and len(embeddings['data']) > 0:
                embed_vector = embeddings['data'][0]['embedding']
                embeds.append(np.array(embed_vector))
            else:
                raise ValueError("åµŒå…¥å­—å…¸ä¸­æ²¡æœ‰æ‰¾åˆ° 'data' æˆ– 'embedding' é”®")
        return np.array(embeds)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    @staticmethod
    def similarity(A, B):
        dot_product = np.dot(A, B)
        norm_A = np.linalg.norm(A)
        norm_B = np.linalg.norm(B)
        cosine_sim = dot_product / (norm_A * norm_B)
        return cosine_sim

    def search(self, query, llm_embed):
        max_similarity = 0
        max_similarity_index = 0
        query_embed = self.embed([query], llm_embed)[0]
        for idx, te in enumerate(self.embeds):
            similarity = self.similarity(query_embed, te)
            if similarity > max_similarity:
                max_similarity = similarity
                max_similarity_index = idx
        return self.docs[max_similarity_index]

############################################################
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'llm_query' not in st.session_state:
    st.session_state.llm_query = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'selected_llm_model' not in st.session_state:
    st.session_state.selected_llm_model = None
if 'n_thread' not in st.session_state:
    st.session_state.n_thread = 8
if 'n_ctx' not in st.session_state:
    st.session_state.n_ctx = 2048
if 'max_tokens' not in st.session_state:
    st.session_state.max_tokens = 512
if 'temperature' not in st.session_state:
    st.session_state.temperature = 0.7
if 'top_p' not in st.session_state:
    st.session_state.top_p = 0.8
if 'top_k' not in st.session_state:
    st.session_state.top_k = 100
if 'llm_config' not in st.session_state:
    st.session_state.llm_config = None

if 'llm_embed' not in st.session_state:
    st.session_state.llm_embed = None
if 'temp_dir' not in st.session_state:
    st.session_state.temp_dir = None
if 'selected_embed_model' not in st.session_state:
    st.session_state.selected_embed_model = None
if 'chunk_size' not in st.session_state:
    st.session_state.chunk_size = 250
if 'chunk_overlap' not in st.session_state:
    st.session_state.chunk_overlap = 50
if 'embed_config' not in st.session_state:
    st.session_state.embed_config = None


def render_llm_config_ui():
    with st.sidebar:
        st.sidebar.header("Select models")
        selected_llm_model = st.selectbox("é€‰æ‹©è¯­è¨€æ¨¡å‹", [model["path"] for model in PRESET_LLM_MODELS], key="llm_model_select")
        st.sidebar.header("Parameters")
        n_thread = st.slider("N_thread", 1, 12, 8, step=1, key="n_thread_slider")
        n_ctx = st.slider("N_ctx", 0, 4096, 2048, step=2, key="n_ctx_slider")
        max_tokens = st.slider("Max_tokens", 0, 1024, 512, step=1, key="max_tokens_slider")
        top_p = st.slider("Top_p", 0.0, 1.0, 0.8, step=0.01, key="top_p_slider")
        top_k = st.slider("Top_k", 0, 100, 100, step=1, key="top_k_slider")
        temperature = st.slider("Temperature", 0.0, 1.0, 0.7, step=0.01, key="temperature_slider")
        st.session_state.selected_llm_model = selected_llm_model
        st.session_state.n_thread = n_thread
        st.session_state.n_ctx = n_ctx
        st.session_state.max_tokens = max_tokens
        st.session_state.top_p = top_p
        st.session_state.top_k = top_k
        st.session_state.temperature = temperature

def get_llm_config():
    selected_llm_model = st.session_state.selected_llm_model
    n_thread = st.session_state.n_thread
    n_ctx = st.session_state.n_ctx
    max_tokens = st.session_state.max_tokens
    top_p = st.session_state.top_p
    top_k = st.session_state.top_k
    temperature = st.session_state.temperature
    llm_config = {
        'n_thread': n_thread,
        'n_ctx': n_ctx,
        'max_tokens': max_tokens,
        'temperature': temperature,
        'top_p': top_p,
        'top_k': top_k,
        'selected_llm_model': selected_llm_model,
    }
    return llm_config

def render_embed_config_ui():
    # è®¾ç½®åˆ’åˆ†chunksæ—¶çš„å‚æ•°
    with st.sidebar:
        st.sidebar.header("Select models")
        selected_embed_model = st.selectbox("é€‰æ‹©Embeddingæ¨¡å‹", [model["path"] for model in PRESET_EMBED_MODELS], key="embed_model_select")
        st.sidebar.header("Split Chunks Parameters")
        chunk_size = st.slider("chunk_size", 50, 500, 250, step=1, key="chunk_size_slider")
        chunk_overlap = st.slider("Chunk_overlap", 0, 100, 50, step=1, key="chunk_overlap_slider")
        st.session_state.selected_embed_model = selected_embed_model
        st.session_state.chunk_size = chunk_size
        st.session_state.chunk_overlap = chunk_overlap

def get_embed_config():
    chunk_size = st.session_state.chunk_size
    chunk_overlap = st.session_state.chunk_overlap
    selected_embed_model = st.session_state.selected_embed_model
    embed_config = {
        'chunk_size': chunk_size,
        'chunk_overlap': chunk_overlap,
        'selected_embed_model': selected_embed_model,
    }
    return embed_config

@st.cache_resource
def init_llm_embed(embed_config):
    llm_embed = Llama(
        model_path=embed_config.get('selected_embed_model'),
        embedding=True,
    )
    return llm_embed

@st.cache_resource
def init_llm_query(llm_config):
    llm_query = Llama(
        model_path=llm_config.get('selected_llm_model'),
        chat_format="chatml",
        n_ctx=llm_config['n_ctx'],
        n_threads=llm_config.get('n_threads'),
    )
    return llm_query




# åˆ›å»º Kb å®ä¾‹
@st.cache_resource
def create_knowledge_base(temp_dir, _llm_embed, chunk_size, chunk_overlap):
    # æ£€æŸ¥ temp_dir æ˜¯å¦ä¸º None
    if temp_dir is None:
        # å¦‚æœ temp_dir æ˜¯ Noneï¼Œå¯ä»¥é€‰æ‹©è¿”å› None æˆ–æ‰§è¡Œå…¶ä»–æ“ä½œ
        return None
    
    # å¦‚æœ temp_dir ä¸æ˜¯ Noneï¼Œç»§ç»­å¤„ç†æ–‡ä»¶è·¯å¾„
    filepaths = [os.path.join(temp_dir, file_name) for file_name in os.listdir(temp_dir)]
    return Kb(filepaths, _llm_embed, chunk_size, chunk_overlap)

# åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæå–å’Œå¤„ç†æ¯ä¸ª chunk ä¸­çš„ content
def extract_content(response):
    generated_text = ""
    for chunk in response:
        if "choices" in chunk and chunk["choices"]:
            delta = chunk["choices"][0].get("delta", {})
            content = delta.get("content", "")
            if content:
                # å»é™¤æ¢è¡Œç¬¦
                # content = content.replace('\n', ' ')
                generated_text += content
                yield content
    return generated_text





st.set_page_config(page_title="RAG Application", page_icon="ğŸ”")
st.title("ğŸ’» Local RAG Chatbot ğŸ¤–")
st.caption("ğŸš€ A RAG chatbot powered by llama-cpp-python ğŸ¦™.")



# æ–‡ä»¶ä¸Šä¼ 
st.sidebar.header("Upload Data")
uploaded_files = st.sidebar.file_uploader("Upload your data files:", type=["txt", "pdf", "docx"], accept_multiple_files=True)
buttonClean = st.sidebar.button("Clear chat history", key="clean")


render_embed_config_ui()
# æ¸²æŸ“å‚æ•°é…ç½® UI
render_llm_config_ui()

# åˆå§‹åŒ–llmæ¨¡å‹ï¼Œå› ä¸ºè¦ä½¿å¾—get_llm_config()è¿”å›å€¼ä¸æ˜¯noneï¼Œæ‰€ä»¥st.session_state.llm_config = get_llm_config()å¿…é¡»æ”¾åˆ°render_llm_config_ui()åé¢
if st.session_state.llm_config is None:
    st.session_state.llm_config = get_llm_config()
    if st.session_state.llm_query is None:
        st.session_state.llm_query = init_llm_query(st.session_state.llm_config)
# åˆå§‹åŒ–embedæ¨¡å‹
if st.session_state.embed_config is None:
    st.session_state.embed_config = get_embed_config()
    if st.session_state.llm_embed is None:
        st.session_state.llm_embed = init_llm_embed(st.session_state.embed_config)

# æ£€æŸ¥å‚æ•°å˜åŒ–è¿›è€Œé‡æ–°åˆå§‹åŒ–æ¨¡å‹
if st.session_state.llm_config != get_llm_config():
    st.session_state.llm_config = get_llm_config()
    if st.session_state.llm_query is not None:
        del st.session_state.llm_query
        st.cache_resource.clear()
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    st.session_state.llm_query = init_llm_query(st.session_state.llm_config)


# åˆå§‹åŒ–æ–‡ä»¶å“ˆå¸Œå€¼
current_files_hash = get_files_hash(uploaded_files) if uploaded_files else None

# æ£€æµ‹æ–‡ä»¶æ˜¯å¦å‘ç”Ÿå˜åŒ–å¹¶åˆå§‹åŒ–æ¨¡å‹
if 'files_hash' in st.session_state:
    if st.session_state['files_hash'] != current_files_hash:
        st.session_state['files_hash'] = current_files_hash
        if uploaded_files:
            st.session_state['temp_dir'] = handle_file_upload(uploaded_files)
            st.sidebar.success("Files uploaded successfully.")
            if st.session_state.llm_embed is None:
                st.session_state.llm_embed = init_llm_embed(st.session_state.embed_config)
            # æ¸…ç†æ—§çš„çŸ¥è¯†åº“å®ä¾‹
            if 'kb' in st.session_state:
                del st.session_state['kb']  # åˆ é™¤æ—§çš„çŸ¥è¯†åº“å®ä¾‹
                # æ¸…ç† create_knowledge_base å‡½æ•°çš„ç¼“å­˜
                create_knowledge_base.clear()
            st.session_state['kb'] = create_knowledge_base(st.session_state['temp_dir'], st.session_state.llm_embed, st.session_state.embed_config.get('chunk_size'), st.session_state.embed_config.get('chunk_overlap'))
        else:
            st.sidebar.error("No uploaded files.")
else:
    if uploaded_files:
        st.session_state['files_hash'] = current_files_hash
        st.session_state['temp_dir'] = handle_file_upload(uploaded_files)
        st.sidebar.success("Files uploaded successfully.")
        if st.session_state.llm_embed is None:
            st.session_state.llm_embed = init_llm_embed(st.session_state.embed_config)
        # æ¸…ç†æ—§çš„çŸ¥è¯†åº“å®ä¾‹
        if 'kb' in st.session_state:
            del st.session_state['kb']  # åˆ é™¤æ—§çš„çŸ¥è¯†åº“å®ä¾‹
            # æ¸…ç† create_knowledge_base å‡½æ•°çš„ç¼“å­˜
            create_knowledge_base.clear()
        st.session_state['kb'] = create_knowledge_base(st.session_state['temp_dir'], st.session_state.llm_embed, st.session_state.embed_config.get('chunk_size'), st.session_state.embed_config.get('chunk_overlap'))
    else:
        st.sidebar.error("No uploaded files.")

# æ£€æŸ¥åµŒå…¥é…ç½®æ˜¯å¦å‘ç”Ÿå˜åŒ–
if st.session_state.embed_config != get_embed_config():
    st.session_state.embed_config = get_embed_config()  # æ›´æ–°é…ç½®

    # æ¸…é™¤æ—§çš„åµŒå…¥æ¨¡å‹å’ŒçŸ¥è¯†åº“å®ä¾‹
    if 'llm_embed' in st.session_state:
        del st.session_state.llm_embed  # åˆ é™¤æ—§çš„åµŒå…¥æ¨¡å‹
    if 'kb' in st.session_state:
        del st.session_state['kb']  # åˆ é™¤æ—§çš„çŸ¥è¯†åº“å®ä¾‹

    # é‡æ–°åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    st.session_state.llm_embed = init_llm_embed(st.session_state.embed_config)

    # å¦‚æœæœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼Œé‡æ–°æ„å»ºçŸ¥è¯†åº“
    if 'temp_dir' in st.session_state and st.session_state['temp_dir'] is not None:
        st.session_state['kb'] = create_knowledge_base(st.session_state['temp_dir'], st.session_state.llm_embed, st.session_state.embed_config.get('chunk_size'), st.session_state.embed_config.get('chunk_overlap'))
    elif 'files_hash' in st.session_state:
        if st.session_state['files_hash'] != current_files_hash:
            st.session_state['files_hash'] = current_files_hash
            if uploaded_files:
                st.session_state['temp_dir'] = handle_file_upload(uploaded_files)
                st.sidebar.success("Files uploaded successfully.")
                st.session_state['kb'] = create_knowledge_base(st.session_state['temp_dir'], st.session_state.llm_embed, st.session_state.embed_config.get('chunk_size'), st.session_state.embed_config.get('chunk_overlap'))
            else:
                st.sidebar.error("No uploaded files.")
    else:
        if uploaded_files:
            st.session_state['files_hash'] = current_files_hash
            st.session_state['temp_dir'] = handle_file_upload(uploaded_files)
            st.sidebar.success("Files uploaded successfully.")
            st.session_state['kb'] = create_knowledge_base(st.session_state['temp_dir'], st.session_state.llm_embed, st.session_state.embed_config.get('chunk_size'), st.session_state.embed_config.get('chunk_overlap'))
        else:
            st.sidebar.error("No uploaded files.")
        

# User and assistant names
U_NAME = "User"
A_NAME = "Assistant"

# Display chat history
for i, message in enumerate(st.session_state.chat_history):
    if message["role"] == "user":
        with st.chat_message(name="user", avatar="user"):
            if message["content"] is not None:
                st.markdown(message["content"])
    else:
        with st.chat_message(name="model", avatar="assistant"):
            st.markdown(message["content"])

if buttonClean:
    st.session_state.chat_history = []
    st.session_state.response = ""
    st.rerun()

# æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ æ–‡ä»¶
if 'kb' not in st.session_state or st.session_state['kb'] is None:
    st.warning("Please upload files to start a chat conversation.")
else:
    # User input box
    user_text = st.chat_input("Enter your question")
    if user_text:
        with st.chat_message(U_NAME, avatar="user"):
            st.session_state.chat_history.append({"role": "user", "content": user_text})
            st.markdown(f"{U_NAME}: {user_text}")

        with st.chat_message(A_NAME, avatar="assistant"):
            # è®¾ç½®Prompt
            prompt_template = """
            ã€æ£€ç´¢å¢å¼ºå‹ç”Ÿæˆï¼ˆRAGï¼‰å¯¹è¯ç³»ç»Ÿ - ç®€ä½“ä¸­æ–‡å›ç­”ã€‘

            ### ç”¨æˆ·é—®é¢˜
            {user_question}

            ### èƒŒæ™¯ä¿¡æ¯
            åŸºäºä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯ï¼š
            {context}

            ### å›ç­”è¦æ±‚
            1. **è¯­è¨€**ï¼šè¯·ä½¿ç”¨æ ‡å‡†ç®€ä½“ä¸­æ–‡ä¹¦å†™ï¼Œé¿å…ç¹ä½“å­—æˆ–æ–¹è¨€ã€‚
            2. **å‡†ç¡®æ€§**ï¼šç¡®ä¿ç­”æ¡ˆåŸºäºæä¾›çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä¸å¼•å…¥æœªç»è¯å®çš„ä¿¡æ¯ã€‚
            3. **å®Œæ•´æ€§**ï¼š
            - å¦‚æœé—®é¢˜æ˜¯å°é—­å¼çš„ï¼ˆå¦‚æ˜¯/å¦é—®é¢˜ï¼‰ï¼Œè¯·ç›´æ¥ç»™å‡ºæ˜ç¡®çš„ç­”æ¡ˆï¼Œå¹¶è§£é‡ŠåŸå› ã€‚
            - å¦‚æœé—®é¢˜æ˜¯å¼€æ”¾å¼çš„ï¼Œè¯·æä¾›è¯¦å°½ä½†ä¸è¿‡äºå†—é•¿çš„å›ç­”ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯ã€‚
            4. **å¼•ç”¨**ï¼šå½“å¼•ç”¨èƒŒæ™¯ä¿¡æ¯ä¸­çš„å…·ä½“å†…å®¹æ—¶ï¼Œè¯·ä½¿ç”¨å¼•å·æ ‡æ³¨ï¼Œå¹¶æŒ‡æ˜å‡ºå¤„æˆ–ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œâ€œæ ¹æ®...çš„æ•°æ®â€ï¼‰ã€‚
            5. **è¡¥å……ä¿¡æ¯**ï¼šå¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œå¯ä»¥æåŠè¿™ä¸€ç‚¹ï¼Œå¹¶è¯´æ˜éœ€è¦å“ªäº›é¢å¤–ä¿¡æ¯æ¥æä¾›æ›´å®Œæ•´çš„ç­”æ¡ˆã€‚
            """

            # åœ¨çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ä¸Šä¸‹æ–‡
            context = st.session_state['kb'].search(user_text, st.session_state.llm_embed)
            # ä½¿ç”¨ str.format() æ–¹æ³•æ¥å¡«å……æ¨¡æ¿
            prompt = prompt_template.format(user_question=user_text, context=context)
            # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
            msgs = [
                {"role": "system", "content": "You are a helpful assistant."},
                {
                    "role": "user",
                    "content": prompt
                }
            ]

            # åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
            response = st.session_state.llm_query.create_chat_completion(
                messages=msgs,
                stream=True,
            )
            # ä½¿ç”¨ st.write_stream è¿›è¡Œæµå¼è¾“å‡º
            generated_text = st.write_stream(extract_content(response))
            # å°†ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬æ·»åŠ åˆ°ä¼šè¯å†å²è®°å½•ä¸­
        st.session_state.chat_history.append({"role": "model", "content": generated_text})

        st.divider()

